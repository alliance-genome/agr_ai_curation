# ============================================================================
# AI Curation Platform - Environment Configuration
# ============================================================================
#
# SETUP (run once):
#
#   make setup
#
# This copies .env.example to ~/.agr_ai_curation/.env with secure permissions.
#
# MANUAL SETUP:
#
#   mkdir -p ~/.agr_ai_curation
#   cp .env.example ~/.agr_ai_curation/.env
#   chmod 600 ~/.agr_ai_curation/.env
#
# USAGE:
#
#   make dev          # Start all services
#   make help         # Show all commands
#
# WHY ~/.agr_ai_curation/?
#
#   Keeping secrets outside the repository prevents accidental commits.
#   The Makefile sources this file and passes variables to Docker.
#
# ============================================================================

# ====================
# API Keys
# ====================

# OpenAI API Key for embeddings and GPT-5/GPT-5-mini
OPENAI_API_KEY=your_openai_api_key_here

# Gemini API Key (required if LLM_PROVIDER=gemini)
# Get your API key from: https://aistudio.google.com/apikey
GEMINI_API_KEY=your_gemini_api_key_here

# Anthropic API Key for Prompt Explorer Opus 4.5 chat (optional)
ANTHROPIC_API_KEY=your_anthropic_api_key_here

# LangFuse Configuration (optional - for observability)
# Self-hosted Langfuse instance
LANGFUSE_SECRET_KEY=your_langfuse_secret_key
LANGFUSE_PUBLIC_KEY=your_langfuse_public_key
LANGFUSE_HOST=http://your-langfuse-host:3000

# ====================
# Weaviate Configuration
# ====================

# Weaviate connection settings
WEAVIATE_HOST=weaviate  # or localhost for local development
WEAVIATE_PORT=8080
WEAVIATE_SCHEME=http  # http or https

# ====================
# PDF Processing Configuration
# ====================

# Docling service URL for PDF processing (required for document upload)
# This is an external service that converts PDFs to structured text
DOCLING_SERVICE_URL=http://your-docling-service:8000

# Docling processing timeout in seconds (increase for large documents)
DOCLING_TIMEOUT=300

# PDF extraction strategy - Controls accuracy vs speed trade-off
# Options: fast (fastest, may miss complex layouts)
#          auto (balanced speed/accuracy)
#          hi_res (best accuracy, slowest)
PDF_EXTRACTION_STRATEGY=fast

# Enable table structure inference
# Note: Enabling this will slow down processing significantly
ENABLE_TABLE_EXTRACTION=false

# ====================
# Processing Configuration
# ====================

# Maximum chunk size in tokens for text splitting
MAX_CHUNK_SIZE=1000

# Overlap between chunks in tokens
CHUNK_OVERLAP=100

# ====================
# Logging Configuration
# ====================

# Log level (DEBUG, INFO, WARNING, ERROR, CRITICAL)
LOG_LEVEL=INFO

# Log details of failed chunk insertions for debugging
LOG_FAILED_CHUNKS=true

# ====================
# Performance Configuration
# ====================

# Batch size for Weaviate operations
WEAVIATE_BATCH_SIZE=100

# Maximum concurrent workers for processing
MAX_WORKERS=4

# Request timeout in seconds
REQUEST_TIMEOUT=300

# ====================
# Feature Flags
# ====================

# Enable fail-fast behavior (recommended for production)
FAIL_FAST=true

# Enable deterministic UUID generation for idempotent operations
DETERMINISTIC_UUIDS=true

# ====================
# Health Check Configuration
# ====================

# Enforce required service health at startup
# When true (default), startup fails if services marked required: true in
# connections.yaml are unhealthy. Set to false for development/testing
# with partial infrastructure.
# IMPORTANT: Keep this true in production!
HEALTH_CHECK_STRICT_MODE=true

# ====================
# Docker Service Credentials
# ====================
# These are used by docker-compose.yml for local services.
# IMPORTANT: Change these from the defaults for any non-local deployment!

# PostgreSQL
POSTGRES_USER=postgres
POSTGRES_PASSWORD=your_postgres_password_here
POSTGRES_DB=ai_curation

# Redis (for Langfuse and cross-worker state)
REDIS_AUTH=your_redis_password_here

# ClickHouse (for Langfuse analytics)
CLICKHOUSE_PASSWORD=your_clickhouse_password_here

# MinIO (for Langfuse object storage)
MINIO_ROOT_USER=minio
MINIO_ROOT_PASSWORD=your_minio_password_here

# Ontology Database
ONTOLOGY_ADMIN_PASSWORD=your_ontology_admin_password_here
ONTOLOGY_READER_PASSWORD=your_ontology_reader_password_here

# ====================
# Database Configuration
# ====================

# Application Database (PostgreSQL)
# Stores: users, pdf_documents, feedback_reports, curation_flows, audit_log, etc.
# Note: Use POSTGRES_PASSWORD variable defined above
DATABASE_URL=postgresql://postgres:${POSTGRES_PASSWORD}@postgres:5432/ai_curation

# External Database URLs (read-only access via SSH tunnel)
# These connect to Alliance production databases for agent queries
# CURATION_DB_URL=postgresql://<user>:<pass>@host.docker.internal:<tunnel_port>/curation
# LITERATURE_DB_URL=postgresql://<user>:<pass>@host.docker.internal:<tunnel_port>/literature

# ====================
# Feedback System SMTP Configuration
# ====================

# SMTP server for sending feedback notifications
SMTP_HOST=smtp.gmail.com
SMTP_PORT=587

# SMTP authentication credentials
SMTP_USERNAME=your_email@gmail.com
SMTP_PASSWORD=your_app_specific_password

# Email sender information
EMAIL_FROM_ADDRESS=your_email@gmail.com
EMAIL_FROM_NAME=AI Curation Feedback System

# Email recipient(s) for feedback notifications
# Can be a single email or comma-separated list
FEEDBACK_RECIPIENT_EMAIL=developer@example.com

# ====================
# AWS Cognito Authentication Configuration (migrated from Okta)
# ====================

# AWS Cognito Region
# Example: us-east-1
COGNITO_REGION=us-east-1

# Cognito User Pool ID
# Example: us-east-1_XXXXXXXXX
COGNITO_USER_POOL_ID=your_user_pool_id_here

# Cognito Application Client ID
COGNITO_CLIENT_ID=your_client_id_here

# Cognito Application Client Secret
COGNITO_CLIENT_SECRET=your_client_secret_here

# Cognito Custom Domain (for Hosted UI)
# This is the domain where Cognito's Hosted UI is hosted
# Default: https://auth.alliancegenome.org
COGNITO_DOMAIN=https://auth.alliancegenome.org

# Cognito Redirect URI (for OAuth callback)
# Default for local development: http://localhost:3002/auth/callback
# Production: https://your-domain.com/auth/callback
COGNITO_REDIRECT_URI=http://localhost:3002/auth/callback

# Session timeout in hours (default: 24 hours)
# Set lower for testing (e.g., 0.01 = 36 seconds)
SESSION_TIMEOUT_HOURS=24

# Secure cookie setting (default: false for local development)
# CRITICAL: Set to true in production to enable HTTPS-only cookies
# When true, cookies will only be sent over HTTPS connections (prevents token interception)
# When false, cookies can be sent over HTTP (for local development only)
SECURE_COOKIES=false

# ====================
# LLM Model Configuration
# ====================

# Embedding model for Vector Store (Weaviate)
# Examples: text-embedding-3-small, text-embedding-3-large
EMBEDDING_MODEL=text-embedding-3-small

# ====================
# LLM Provider Configuration
# ====================
# Select which LLM provider to use for all agents.
# Options: openai (default), gemini
#
# When using Gemini, models are accessed via OpenAI compatibility mode.
# The GEMINI_API_KEY must be set when LLM_PROVIDER=gemini.
#
# Supported models by provider:
#   OpenAI: gpt-5, gpt-5-mini
#   Gemini: gemini-3-pro-preview (with "low"/"high" thinking levels)
#
LLM_PROVIDER=openai

# ====================
# Default Agent Settings
# ====================
# These are the master defaults - all agents inherit these unless overridden.
# Change here to switch all agents at once.

# Default model for all agents (gpt-5-mini recommended for cost/performance)
DEFAULT_AGENT_MODEL=gpt-5-mini

# Temperature: Only used by Gemini models. GPT-5 ignores temperature when reasoning is enabled.
# Gemini 3 Pro requires temperature >= 1.0 (LiteLLM warning)
# Uncomment and set when using Gemini:
#DEFAULT_AGENT_TEMPERATURE=1.0

# Reasoning effort: minimal, low, medium, high
# - GPT-5: Uses all four levels directly (default: medium per OpenAI)
# - Gemini 3: Maps minimal/low → "low" thinking, medium/high → "high" thinking
# Higher = better quality but slower/more expensive
DEFAULT_AGENT_REASONING=low

# Supervisor uses higher reasoning by default (complex routing decisions)
DEFAULT_SUPERVISOR_REASONING=medium

# ====================
# OpenAI Agents Configuration
# ====================
# Each agent can be configured individually via environment variables.
# If not set, agents inherit from DEFAULT_AGENT_* settings above.
#
# Naming convention: AGENT_{AGENT_NAME}_{SETTING}
#
# Available settings per agent:
#   MODEL       - Model name (e.g., gpt-5-mini, gemini-3-pro-preview)
#   TEMPERATURE - Temperature (0.0 to 1.0) - only used by Gemini, GPT-5 ignores this
#   REASONING   - Reasoning effort (minimal, low, medium, high)
#   TOOL_CHOICE - Tool selection mode (required, auto, or specific tool name)

# --- Global Runner Setting ---
AGENT_MAX_TURNS=20

# ====================
# Per-Agent Overrides (optional)
# ====================
# Uncomment to override defaults for specific agents.

# --- Supervisor Agent ---
#AGENT_SUPERVISOR_MODEL=gpt-5-mini
#AGENT_SUPERVISOR_TEMPERATURE=0.1
#AGENT_SUPERVISOR_REASONING=medium

# --- PDF Specialist ---
#AGENT_PDF_MODEL=gpt-5-mini
#AGENT_PDF_TEMPERATURE=0.3
#AGENT_PDF_REASONING=low
#AGENT_PDF_TOOL_CHOICE=required

# --- Gene Expression Specialist ---
#AGENT_GENE_EXPRESSION_MODEL=gpt-5-mini
#AGENT_GENE_EXPRESSION_TEMPERATURE=0.3
#AGENT_GENE_EXPRESSION_REASONING=low
#AGENT_GENE_EXPRESSION_TOOL_CHOICE=required

# --- Gene Curation Specialist ---
#AGENT_GENE_MODEL=gpt-5-mini
#AGENT_GENE_TEMPERATURE=0.3
#AGENT_GENE_REASONING=low
#AGENT_GENE_TOOL_CHOICE=agr_curation_query

# --- Allele Curation Specialist ---
#AGENT_ALLELE_MODEL=gpt-5-mini
#AGENT_ALLELE_TEMPERATURE=0.3
#AGENT_ALLELE_REASONING=low
#AGENT_ALLELE_TOOL_CHOICE=agr_curation_query

# --- Disease Ontology Specialist ---
#AGENT_DISEASE_MODEL=gpt-5-mini
#AGENT_DISEASE_TEMPERATURE=0.3
#AGENT_DISEASE_REASONING=low
#AGENT_DISEASE_TOOL_CHOICE=disease_ontology_sql

# --- Chemical Ontology Specialist ---
#AGENT_CHEMICAL_MODEL=gpt-5-mini
#AGENT_CHEMICAL_TEMPERATURE=0.3
#AGENT_CHEMICAL_REASONING=low
#AGENT_CHEMICAL_TOOL_CHOICE=chebi_api_call

# --- Gene Ontology (QuickGO) Specialist ---
#AGENT_GENE_ONTOLOGY_MODEL=gpt-5-mini
#AGENT_GENE_ONTOLOGY_TEMPERATURE=0.3
#AGENT_GENE_ONTOLOGY_REASONING=low
#AGENT_GENE_ONTOLOGY_TOOL_CHOICE=quickgo_api_call

# --- GO Annotations Specialist ---
#AGENT_GO_ANNOTATIONS_MODEL=gpt-5-mini
#AGENT_GO_ANNOTATIONS_TEMPERATURE=0.3
#AGENT_GO_ANNOTATIONS_REASONING=low
#AGENT_GO_ANNOTATIONS_TOOL_CHOICE=alliance_api_call

# --- Alliance Orthologs Specialist ---
#AGENT_ORTHOLOGS_MODEL=gpt-5-mini
#AGENT_ORTHOLOGS_TEMPERATURE=0.3
#AGENT_ORTHOLOGS_REASONING=low
#AGENT_ORTHOLOGS_TOOL_CHOICE=alliance_api_call

# --- Ontology Mapping Specialist ---
#AGENT_ONTOLOGY_MAPPING_MODEL=gpt-5-mini
#AGENT_ONTOLOGY_MAPPING_TEMPERATURE=0.3
#AGENT_ONTOLOGY_MAPPING_REASONING=low
#AGENT_ONTOLOGY_MAPPING_TOOL_CHOICE=agr_curation_query

# ============================================================================
# Trace Review (Agent Studio) Configuration
# ============================================================================
# The trace_review service uses TRACE_REVIEW_* prefixed variables because
# it connects to a DIFFERENT Langfuse instance (Alliance internal) than
# the main app's local Langfuse.
#
# If you're not using trace_review, you can ignore this section.
# For trace_review setup, you can also use a separate file at:
#   ~/.agr_ai_curation/trace_review/.env
# ============================================================================

# Trace Review - Development Mode
# Set to true for local development without Cognito auth
TRACE_REVIEW_DEV_MODE=true

# Trace Review - Langfuse (Alliance internal instance, requires VPN)
# This is DIFFERENT from the main app's local Langfuse
TRACE_REVIEW_LANGFUSE_HOST=http://langfuse-internal.alliancegenome.org:3000
TRACE_REVIEW_LANGFUSE_PUBLIC_KEY=
TRACE_REVIEW_LANGFUSE_SECRET_KEY=

# Trace Review - Local Langfuse alternative (optional)
# Use source="local" in API calls to use this instead
TRACE_REVIEW_LANGFUSE_LOCAL_HOST=http://localhost:3000
TRACE_REVIEW_LANGFUSE_LOCAL_PUBLIC_KEY=
TRACE_REVIEW_LANGFUSE_LOCAL_SECRET_KEY=

# Trace Review - Server config (usually don't need to change)
TRACE_REVIEW_BACKEND_HOST=localhost
TRACE_REVIEW_BACKEND_PORT=8001
TRACE_REVIEW_FRONTEND_URL=http://localhost:3001
TRACE_REVIEW_CACHE_TTL_HOURS=1

# Trace Review - Cognito (uses main COGNITO_* by default, override here if different)
#TRACE_REVIEW_COGNITO_REGION=us-east-1
#TRACE_REVIEW_COGNITO_USER_POOL_ID=
#TRACE_REVIEW_COGNITO_CLIENT_ID=
#TRACE_REVIEW_COGNITO_CLIENT_SECRET=
#TRACE_REVIEW_COGNITO_DOMAIN=
#TRACE_REVIEW_COGNITO_REDIRECT_URI=http://localhost:3001/api/auth/callback